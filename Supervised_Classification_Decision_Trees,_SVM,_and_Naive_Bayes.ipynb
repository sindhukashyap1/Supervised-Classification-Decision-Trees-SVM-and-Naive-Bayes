{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: What is Information Gain, and how is it used in Decision Trees?\n",
        "# Information Gain is a metric used to measure how well a feature separates the training\n",
        "# examples into target classes. It is based on the concept of entropy, which measures the\n",
        "# level of uncertainty or impurity in a dataset.\n",
        "#\n",
        "# In Decision Trees, Information Gain is used to decide which feature should be selected\n",
        "# at each split. The feature that results in the highest reduction in entropy (highest\n",
        "# information gain) is chosen, as it best separates the data into homogeneous groups.\n",
        "\n",
        "\n",
        "# Q2: What is the difference between Gini Impurity and Entropy?\n",
        "# Gini Impurity and Entropy are both measures of node impurity used in Decision Trees.\n",
        "#\n",
        "# Entropy:\n",
        "# - Measures the level of randomness or uncertainty in the data.\n",
        "# - Based on logarithmic calculations.\n",
        "# - More computationally expensive.\n",
        "# - Used in ID3 and C4.5 algorithms.\n",
        "#\n",
        "# Gini Impurity:\n",
        "# - Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "# - Computationally faster than entropy.\n",
        "# - Used in CART algorithms.\n",
        "#\n",
        "# In practice, both produce similar trees, but Gini is preferred for faster computation.\n",
        "\n",
        "\n",
        "# Q3: What is Pre-Pruning in Decision Trees?\n",
        "# Pre-pruning is a technique used to prevent overfitting in Decision Trees by stopping\n",
        "# the tree growth early.\n",
        "#\n",
        "# Instead of growing the tree fully, constraints are applied during training, such as:\n",
        "# - Maximum depth of the tree\n",
        "# - Minimum number of samples required to split a node\n",
        "# - Minimum samples per leaf\n",
        "#\n",
        "# Pre-pruning helps improve generalization and reduces model complexity.\n",
        "\n",
        "\n",
        "# Q5: What is a Support Vector Machine (SVM)?\n",
        "# Support Vector Machine is a supervised machine learning algorithm used for both\n",
        "# classification and regression tasks.\n",
        "#\n",
        "# SVM works by finding an optimal hyperplane that best separates data points of different\n",
        "# classes with the maximum margin. The data points closest to the hyperplane are called\n",
        "# support vectors and play a crucial role in defining the decision boundary.\n",
        "\n",
        "\n",
        "# Q6: What is the Kernel Trick in SVM?\n",
        "# The Kernel Trick is a technique used in SVM to handle non-linearly separable data.\n",
        "#\n",
        "# It transforms the input data into a higher-dimensional feature space where a linear\n",
        "# separation becomes possible, without explicitly computing the transformation.\n",
        "#\n",
        "# Common kernels include:\n",
        "# - Linear\n",
        "# - Polynomial\n",
        "# - Radial Basis Function (RBF)\n",
        "# - Sigmoid\n",
        "\n",
        "\n",
        "# Q8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "# Naïve Bayes is a probabilistic classification algorithm based on Bayes’ Theorem.\n",
        "#\n",
        "# It is called \"Naïve\" because it assumes that all features are conditionally independent\n",
        "# of each other given the class label, which is rarely true in real-world data.\n",
        "#\n",
        "# Despite this strong assumption, Naïve Bayes performs well in many applications such as\n",
        "# text classification and spam detection.\n",
        "\n",
        "\n",
        "# Q9: Explain the differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "# Gaussian Naïve Bayes:\n",
        "# - Assumes features follow a normal distribution.\n",
        "# - Used for continuous data.\n",
        "#\n",
        "# Multinomial Naïve Bayes:\n",
        "# - Used for discrete count data.\n",
        "# - Commonly applied in text classification with word counts.\n",
        "#\n",
        "# Bernoulli Naïve Bayes:\n",
        "# - Works with binary features (0 or 1).\n",
        "# - Suitable when presence or absence of a feature matters.\n"
      ],
      "metadata": {
        "id": "Vv4O32TO_nAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: Train a Decision Tree Classifier using Gini Impurity and store feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n"
      ],
      "metadata": {
        "id": "XmDGkpbp_q9m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: Train Linear and RBF SVM classifiers on Wine dataset and compare accuracies\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "linear_accuracy = svm_linear.score(X_test, y_test)\n",
        "rbf_accuracy = svm_rbf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "F-LapYjd_3TC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10: Train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "accuracy = model.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "9CDgVH5iAHNS"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}